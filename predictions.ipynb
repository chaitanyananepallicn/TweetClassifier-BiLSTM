{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "400dcf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c70751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3849d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 {'0': 'hate', '1': 'offensive', '2': 'neither'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "max_len = config['max_len']\n",
    "class_labels = config['class_labels']\n",
    "\n",
    "print(max_len, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2bedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196974f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Initialize SymSpell object\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "\n",
    "# Load the frequency dictionary\n",
    "dictionary_path = r\"C:\\Users\\91781\\OneDrive\\Desktop\\MLProjects\\datasets\\fdsymspelly.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def correct_text(text):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        # Skip emoji tokens\n",
    "        if word.startswith(\"EMOJI_\"):\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup_compound(word, max_edit_distance=2)\n",
    "            if suggestions:\n",
    "                corrected_words.append(suggestions[0].term)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "293ffe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text_data(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Convert HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Replace emojis with EMOJI_<name> tokens\n",
    "    text = emoji.demojize(text)  # \":face_with_tears_of_joy:\" etc.\n",
    "    text = re.sub(r\":([a-zA-Z0-9_]+):\", r\" EMOJI_\\1 \", text)  # Preserve as single token\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+)\", \" \", text)\n",
    "\n",
    "    # Remove mentions and RT\n",
    "    text = re.sub(r\"(@\\w+|rt)\", \" \", text)\n",
    "\n",
    "    # Remove hashtags (optional)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "\n",
    "    # Correct spellings\n",
    "    text = correct_text(text)\n",
    "\n",
    "    # Remove punctuation except underscore (EMOJI tokens use underscore)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9_\\s]\", \" \", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    # Rejoin\n",
    "    return \" \".join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e2da29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    # Hate\n",
    "    \"I can't stand those people, they ruin everything!\",\n",
    "    \"These losers should just disappear from the internet forever, total trash\",\n",
    "    \"People like them make me sick, they should be erased from society.\",\n",
    "    \"you're fucking gay, blacklisted hoe\"\" Holding out for #TehGodClan anyway http://t.co/xUCcwoetmn\",\n",
    "    \"I hate those idiots üò° they ruin everything!\"\n",
    "    \"People like him should just disappear from this world üíÄ\",\n",
    "    \"I can't stand those losers üò§, they make me sick!\",\n",
    "    \"These trash humans have no place here üëé\",\n",
    "    \"I despise everyone who supports that garbage ü§Æ\",\n",
    "    \n",
    "    # Offensive\n",
    "    \"fuck no that bitch dont even suck dick üòÜüòÜüòÜ the Kermit videos bout to fuck IG up\",\n",
    "    \"got ya bitch tip toeing on my hardwood floors üòÜ http://t.co/cOU2WQ5L4q\",\n",
    "    \"fuck that guy, he can rot in hell üòí\",\n",
    "    \n",
    "    # Neither\n",
    "    \"Going for a walk in the park today, feeling so relaxed and happy\",\n",
    "    \"I just watched a movie and it was amazing, highly recommend it to everyone\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bbebbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stand people ruin everything',\n",
       " 'loser disappear internet forever total trash',\n",
       " 'people like make sick erased society',\n",
       " 'fucking gay blacklisted hoe holding ocean anyway',\n",
       " 'hate idiot EMOJI_enraged_face ruin everything people like disappear world EMOJI_skull',\n",
       " 'stand loser EMOJI_face_with_steam_from_nose make sick',\n",
       " 'trash human place EMOJI_thumbs_down',\n",
       " 'despise everyone supp garbage EMOJI_face_vomiting',\n",
       " 'fuck bitch done even suck dick EMOJI_grinning_squinting_face EMOJI_grinning_squinting_face EMOJI_grinning_squinting_face kermit video bout fuck',\n",
       " 'got bitch tip toeing hardwood floor EMOJI_grinning_squinting_face',\n",
       " 'fuck guy rot hell EMOJI_unamused_face',\n",
       " 'going walk park today feeling relaxed happy',\n",
       " 'watched movie amazing highly recommend everyone']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets = [clean_text_data(tweet) for tweet in tweets]\n",
    "preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d900a300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[350, 44, 2844, 312],\n",
       " [1307, 6925, 995, 744, 1463, 14],\n",
       " [44, 4, 24, 416, 10564, 3301],\n",
       " [15, 167, 7579, 2, 1667, 2043, 723],\n",
       " [38, 1301, 489, 2844, 312, 44, 4, 6925, 227, 163],\n",
       " [350, 1307, 680, 24, 416],\n",
       " [14, 938, 380, 1241],\n",
       " [6054, 301, 458, 1126],\n",
       " [8, 1, 43, 49, 188, 58, 1825, 1825, 1825, 4609, 369, 61, 8],\n",
       " [10, 1, 884, 4610, 7566, 1051, 1825],\n",
       " [8, 105, 5119, 183, 100],\n",
       " [91, 356, 736, 124, 300, 6785, 149],\n",
       " [1326, 422, 1319, 2671, 4326, 301]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(preprocessed_tweets)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1560d2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,    44,  2844,   312],\n",
       "       [    0,     0,     0, ...,   744,  1463,    14],\n",
       "       [    0,     0,     0, ...,   416, 10564,  3301],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  5119,   183,   100],\n",
       "       [    0,     0,     0, ...,   300,  6785,   149],\n",
       "       [    0,     0,     0, ...,  2671,  4326,   301]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "input_sequences=np.array(pad_sequences(sequences,maxlen=max_len,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61594b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 751ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.8190141e-02, 6.5621728e-01, 3.0559263e-01],\n",
       "       [2.4761833e-02, 8.7388575e-02, 8.8784957e-01],\n",
       "       [9.3405806e-02, 3.2226607e-01, 5.8432811e-01],\n",
       "       [3.8746339e-01, 6.0858566e-01, 3.9508999e-03],\n",
       "       [8.1408903e-02, 7.1803296e-01, 2.0055816e-01],\n",
       "       [5.9920214e-02, 6.4313376e-01, 2.9694602e-01],\n",
       "       [1.5731899e-01, 2.1676639e-01, 6.2591463e-01],\n",
       "       [7.2147250e-02, 7.6640093e-01, 1.6145182e-01],\n",
       "       [1.6826520e-02, 9.8317242e-01, 1.0813905e-06],\n",
       "       [8.3327359e-03, 9.9148059e-01, 1.8666331e-04],\n",
       "       [7.1245737e-02, 9.1828722e-01, 1.0467026e-02],\n",
       "       [7.1846410e-03, 2.0224769e-01, 7.9056770e-01],\n",
       "       [2.3608976e-03, 2.7406814e-02, 9.7023231e-01]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(input_sequences)\n",
    "pred_classes=np.argmax(pred,axis=1)\n",
    "pred_labels = [class_labels[str(i)] for i in pred_classes]\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e361282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I can't stand those people, they ruin everything!\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: These losers should just disappear from the internet forever, total trash\n",
      "Predicted Label: neither\n",
      "--------------------------------------------------\n",
      "Tweet: People like them make me sick, they should be erased from society.\n",
      "Predicted Label: neither\n",
      "--------------------------------------------------\n",
      "Tweet: you're fucking gay, blacklisted hoe Holding out for #TehGodClan anyway http://t.co/xUCcwoetmn\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: I hate those idiots üò° they ruin everything!People like him should just disappear from this world üíÄ\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: I can't stand those losers üò§, they make me sick!\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: These trash humans have no place here üëé\n",
      "Predicted Label: neither\n",
      "--------------------------------------------------\n",
      "Tweet: I despise everyone who supports that garbage ü§Æ\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: fuck no that bitch dont even suck dick üòÜüòÜüòÜ the Kermit videos bout to fuck IG up\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: got ya bitch tip toeing on my hardwood floors üòÜ http://t.co/cOU2WQ5L4q\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: fuck that guy, he can rot in hell üòí\n",
      "Predicted Label: offensive\n",
      "--------------------------------------------------\n",
      "Tweet: Going for a walk in the park today, feeling so relaxed and happy\n",
      "Predicted Label: neither\n",
      "--------------------------------------------------\n",
      "Tweet: I just watched a movie and it was amazing, highly recommend it to everyone\n",
      "Predicted Label: neither\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for tweet, label in zip(tweets, pred_labels):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Predicted Label: {label}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
